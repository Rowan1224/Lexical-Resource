{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from collections import Counter\n",
    "import gzip\n",
    "import shutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def clean(text):\n",
    "    p = re.compile('\"|,|\\[|\\]|')\n",
    "    cleaned = p.sub('',text)\n",
    "    cleaned= cleaned.replace(\"'\", \"\")\n",
    "    return cleaned.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266231f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ranking(model_scores):\n",
    "    model_scores = sorted(model_scores.items(), key=lambda x:x[1])\n",
    "    return {key_value[0]:rank for rank,key_value in enumerate(model_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99926e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_majority_rank(model_tags_map, scores):\n",
    "    \n",
    "    tags = list(model_tags_map.values())\n",
    "#     model_tags_map = x.to_dict()\n",
    "    scores = model_ranking(scores)\n",
    "    votes = {}\n",
    "    for model, tag in model_tags_map.items():\n",
    "        try:\n",
    "            votes[tag] += scores[model]\n",
    "        except:\n",
    "            votes[tag] = scores[model]\n",
    "    \n",
    "    mx = 0\n",
    "    winner = 'O'\n",
    "    for key, val in votes.items():\n",
    "        if val>mx:\n",
    "            winner = key\n",
    "            mx = val\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_majority_weighted(model_tags_map, scores):\n",
    "    \n",
    "    tags = list(model_tags_map.values())\n",
    "    \n",
    "    votes = {}\n",
    "    for model, tag in model_tags_map.items():\n",
    "        try:\n",
    "            votes[tag] += scores[model]\n",
    "        except:\n",
    "            votes[tag] = scores[model]\n",
    "    \n",
    "    mx = 0\n",
    "    winner = 'O'\n",
    "    for key, val in votes.items():\n",
    "        if val>mx:\n",
    "            winner = key\n",
    "            mx = val\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_majority(x, scores):\n",
    "    \n",
    "#     print(x)\n",
    "    tags = list(x.values())\n",
    "    votes = Counter(tags)\n",
    "    mx = 0\n",
    "    winner = 'O'\n",
    "    for key, val in votes.items():\n",
    "        if val>mx:\n",
    "            winner = key\n",
    "            mx = val\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x, func ,scores):\n",
    "    model_tags_map = x.to_dict()\n",
    "    length = len(list(model_tags_map.values())[0])\n",
    "    output = []\n",
    "    for i in range(length):\n",
    "        preds = {model:model_tags_map[model][i] for model in model_tags_map.keys()}\n",
    "        output.append(func(preds, scores))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(lang, eval_set):\n",
    "    \n",
    "    dir = f'./Test-Dev results/{lang}/{eval_set}'\n",
    "    files = os.listdir(dir)\n",
    "    files = [f for f in files if f.endswith('.csv')]\n",
    "    dfs = [pd.read_csv(os.path.join(dir,f))  for f in files]\n",
    "    \n",
    "    merged = pd.DataFrame()\n",
    "    for i in range(len(dfs)):\n",
    "        col = f'preds{i}'\n",
    "        merged[col] = dfs[i].predictions\n",
    "        merged[col] = merged[col].parallel_apply(lambda x: x.split())\n",
    "    \n",
    "    if eval_set=='dev':\n",
    "        merged['true'] = dfs[0].true\n",
    "        merged['true'] = merged['true'].parallel_apply(lambda x: clean(x))\n",
    "        true = reduce(add, merged['true'])\n",
    "        merged = merged.drop(columns=['true'])\n",
    "        \n",
    "    else:\n",
    "        true = None\n",
    "\n",
    "    cols = [f.replace('outputs-','').replace('.csv','') for f in files]\n",
    "    \n",
    "    merged.columns = cols\n",
    "\n",
    "\n",
    "    return merged, true, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ca84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_f1(df, true, cols):\n",
    "    \n",
    "    preds = [reduce(add, df[col].array) for col in cols]\n",
    "#     print(\"Done Reducing\")\n",
    "    \n",
    "    df = pd.DataFrame(preds)\n",
    "    df = df.T\n",
    "    df.columns = cols\n",
    "#     print(df.head())\n",
    "    best = 0\n",
    "    scores = {}\n",
    "    for col in cols:\n",
    "        f1 = classification_report(df[col], true, output_dict=True,zero_division=1)['macro avg']['f1-score']\n",
    "        scores[col] = f1\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed70a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(dataframe, true, func, scores, eval_set='test'):\n",
    "    \n",
    "    ens = dataframe.parallel_apply(lambda x: combine(x, func ,scores), axis=1)\n",
    "    \n",
    "    if eval_set=='dev':\n",
    "        preds = reduce(add, ens)\n",
    "        f1 = classification_report(preds, true, output_dict=True,zero_division=1)['macro avg']['f1-score']\n",
    "    else:\n",
    "        f1 = None\n",
    "    \n",
    "    return ens, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_scores(lang):\n",
    "    \n",
    "\n",
    "    df, true, cols = merge_df(lang, 'dev')\n",
    "    \n",
    "    scores = Counter(best_f1(df, true, cols))\n",
    "    scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    \n",
    "    model, best = scores[0]\n",
    "    best = best*100\n",
    "    \n",
    "    scores = {model:f1 for model,f1 in scores}\n",
    "    \n",
    "    return scores, model, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad528a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'\n",
    "eval_set = 'test'\n",
    "print(lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b79238",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, model, best = get_dev_scores(lang)\n",
    "\n",
    "df, true, dfs = merge_df(lang, eval_set)\n",
    "\n",
    "majority, ef1 = ensemble(df, true, ensemble_majority, scores, eval_set)\n",
    "\n",
    "weighted, ef2 = ensemble(df, true, ensemble_majority_weighted, scores, eval_set)\n",
    "\n",
    "rank, ef3 = ensemble(df, true, ensemble_majority_rank, scores,  eval_set)\n",
    "\n",
    "\n",
    "print(f'{lang}: best_f1 = {best*100:.2f} majority: {ef1*100:00.2f} weighted: {ef2*100:00.2f} rank: {ef3*100:00.2f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
